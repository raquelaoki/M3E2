{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "official-solid",
   "metadata": {},
   "source": [
    "# M3E2\n",
    "\n",
    "## Author: Raquel Aoki\n",
    "\n",
    "Date: Spring 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acoustic-junior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ParKCa' already exists and is not an empty directory.\n",
      "Cloning into 'CompBioAndSimulated_Datasets'...\n",
      "remote: Enumerating objects: 123, done.\u001b[K\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 123 (delta 74), reused 77 (delta 35), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (123/123), 129.39 KiB | 939.00 KiB/s, done.\n",
      "Resolving deltas: 100% (74/74), done.\n",
      "fatal: destination path 'bartpy' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/raquelaoki/ParKCa.git\n",
    "!git clone https://github.com/raquelaoki/CompBioAndSimulated_Datasets.git\n",
    "!git clone https://github.com/JakeColtman/bartpy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twenty-substance",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_assert_shallow_structure' from 'tree' (bartpy/tree.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0840f39f50f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bartpy/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ParKCa/src/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mParKCa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCompBioAndSimulated_Datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated_data_multicause\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_m3e2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm3e2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/ParKCa/src/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfd\u001b[0m  \u001b[0;31m# conda install -c conda-forge tensorflow-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# `python/__init__.py` as necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/substrates/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;31m# Non-lazy load of packages that register with tensorflow or keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.7.7/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/experimental/bijectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability experimental bijectors package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldj_ratio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minverse_log_det_jacobian_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_function_with_inferred_inverse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScalarFunctionWithInferredInverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_value\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbsoluteValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_linear_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffineLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/absolute_value.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/bijector.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mname_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefer_static\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/prefer_static.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnptf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Try catch required to avoid breaking Probability opensource presubmits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/backend/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/backend/numpy/bitwise.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m __all__ = [\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/backend/numpy/_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/6007580/raoki/M3E2/env/lib/python3.7/site-packages/tensorflow_probability/python/internal/backend/numpy/nest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_assert_shallow_structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_DOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IF_SHALLOW_IS_SEQ_INPUT_MUST_BE_SEQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_assert_shallow_structure' from 'tree' (bartpy/tree.py)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import tensorflow_probability as t\n",
    "sys.path.insert(0, 'src/')\n",
    "sys.path.insert(0, 'bartpy/')  # https://github.com/JakeColtman/bartpy\n",
    "sys.path.insert(0, 'bartpy/')\n",
    "sys.path.insert(0, 'ParKCa/src/')\n",
    "from ParKCa.src.train import *\n",
    "from CompBioAndSimulated_Datasets.simulated_data_multicause import *\n",
    "import model_m3e2 as m3e2\n",
    "\n",
    "\n",
    "def main(config_path, seed_models, seed_data):\n",
    "    \"\"\"Start: Parameters Loading\"\"\"\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    params = config['parameters']\n",
    "\n",
    "    # Fix numpy seed for reproducibility\n",
    "    np.random.seed(seed_models)\n",
    "    # Fix random seed for reproducibility\n",
    "    random.seed(seed_models)\n",
    "    # Fix Torch graph-level seed for reproducibility\n",
    "    torch.manual_seed(seed_models)\n",
    "\n",
    "    if 'gwas' in params['data']:\n",
    "\n",
    "        params_b = {'DA': {'k': [15]},\n",
    "                    'CEVAE': {'num_epochs': 100, 'batch': 200, 'z_dim': 10}}\n",
    "\n",
    "        params[\"n_treatments\"] = trykey(params, 'n_treatments', 5)\n",
    "        prop = params[\"n_treatments\"] / (params[\"n_treatments\"] + params['n_covariates'])\n",
    "\n",
    "        sdata_gwas = gwas_simulated_data(prop_tc=prop,\n",
    "                                         pca_path=arg['path']+'CompBioAndSimulated_Datasets/data/tgp_pca2.txt',\n",
    "                                         seed=seed_data,\n",
    "                                         n_units=params['n_sample'],\n",
    "                                         n_causes=params[\"n_treatments\"] + params['n_covariates'])\n",
    "        X, y, y01, treatement_columns, treatment_effects, group = sdata_gwas.generate_samples()\n",
    "        # Train and Test split use the same seed\n",
    "        params['baselines'] = trykey(params, 'baselines', False)\n",
    "        if params['baselines']:\n",
    "            baselines_results, exp_time, f1_test = baselines(params['baselines_list'], pd.DataFrame(X), y01, params_b,\n",
    "                                                             TreatCols=treatement_columns, timeit=True,\n",
    "                                                             seed=seed_models)\n",
    "        else:\n",
    "            baselines_results, exp_time, f1_test = baselines(['noise'], pd.DataFrame(X), y01, params_b,\n",
    "                                                             TreatCols=treatement_columns, timeit=True,\n",
    "                                                             seed=seed_models)\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y01, test_size=0.33, random_state=seed_models)\n",
    "        print('... Target - proportion of 1s', np.sum(y01) / len(y01))\n",
    "        # Split X1, X2 on GWAS: case with no clinicla variables , X2 = X\n",
    "        X1_cols = []\n",
    "        X2_cols = range(X.shape[1] - len(treatement_columns))\n",
    "\n",
    "        data_nnl = m3e2.data_nn(X_train.values, X_test.values, y_train, y_test, treatement_columns,\n",
    "                                treatment_effects[treatement_columns], X1_cols, X2_cols)\n",
    "        loader_train, loader_val, loader_test, num_features = data_nnl.loader(params['suffle'], params['batch_size'],\n",
    "                                                                              seed_models)\n",
    "        params['pos_weights'] = data_nnl.treat_weights\n",
    "        params['pos_weight_y'] = trykey(params, 'pos_weight_y', 1)\n",
    "        params['hidden1'] = trykey(params, 'hidden1', 64)\n",
    "        params['hidden2'] = trykey(params, 'hidden2', 8)\n",
    "        cate_m3e2, f1_test_ = m3e2.fit_nn(loader_train, loader_val, loader_test, params, treatement_columns,\n",
    "                                          num_features,\n",
    "                                          X1_cols, X2_cols)\n",
    "        print('... CATE')\n",
    "        baselines_results['M3E2'] = cate_m3e2\n",
    "        exp_time['M3E2'] = time.time() - start_time\n",
    "        f1_test['M3E2'] = f1_test_\n",
    "        output = organize_output(baselines_results.copy(), treatment_effects[treatement_columns], exp_time, f1_test)\n",
    "    if 'copula' in params['data']:\n",
    "        params_b = {'DA': {'k': [5]},\n",
    "                    'CEVAE': {'num_epochs': 100, 'batch': 200, 'z_dim': 5}}\n",
    "\n",
    "        sdata_copula = copula_simulated_data(seed=seed_data, n=params['n_sample'], s=params['n_covariates'])\n",
    "        X, y, y01, treatement_columns, treatment_effects = sdata_copula.generate_samples()\n",
    "\n",
    "        if params['baselines']:\n",
    "            baselines_results, exp_time, f1_test = baselines(params['baselines_list'], pd.DataFrame(X), y01, params_b,\n",
    "                                                             TreatCols=treatement_columns, timeit=True,\n",
    "                                                             seed=seed_models)\n",
    "        else:\n",
    "            baselines_results, exp_time, f1_test = baselines(['noise'], pd.DataFrame(X), y01, params_b,\n",
    "                                                             TreatCols=treatement_columns, timeit=True,\n",
    "                                                             seed=seed_models)\n",
    "        start = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y01, test_size=0.33, random_state=seed_models)\n",
    "        X1_cols = []\n",
    "        X2_cols = range(X.shape[1] - len(treatement_columns))\n",
    "        # TODO: add other baselines here to run everything on the same train/testing sets\n",
    "\n",
    "        data_nnl = m3e2.data_nn(X_train, X_test, y_train, y_test, treatement_columns,\n",
    "                                treatment_effects, X1_cols, X2_cols)\n",
    "        loader_train, loader_val, loader_test, num_features = data_nnl.loader(params['suffle'], params['batch_size'],\n",
    "                                                                              seed_models)\n",
    "        params['pos_weights'] = data_nnl.treat_weights\n",
    "        params['pos_weight_y'] = trykey(params, 'pos_weight_y', 1)\n",
    "        params['hidden1'] = trykey(params, 'hidden1', 6)\n",
    "        params['hidden2'] = trykey(params, 'hidden2', 6)\n",
    "\n",
    "        cate_m3e2, f1_test_ = m3e2.fit_nn(loader_train, loader_val, loader_test, params, treatement_columns,\n",
    "                                          num_features,\n",
    "                                          X1_cols, X2_cols)\n",
    "        print('... CATE')\n",
    "        cate = pd.DataFrame({'CATE_M3E2': cate_m3e2, 'True_Effect': treatment_effects})\n",
    "        baselines_results['M3E2'] = cate_m3e2\n",
    "        exp_time['M3E2'] = time.time() - start_time\n",
    "        f1_test['M3E2'] = f1_test_\n",
    "        output = organize_output(baselines_results.copy(), treatment_effects[treatement_columns], exp_time, f1_test)\n",
    "    if 'gwas' not in params['data'] and 'copula' not in params['data']:\n",
    "        print(\n",
    "            \"ERRROR! \\nDataset not recognized. \\nChange the parameter data in your config.yaml file to gwas or copula.\")\n",
    "\n",
    "    name = 'output_' + params['data'][0] + '_' + params['id'] + '.csv'\n",
    "    output['seed_data'] = seed_data\n",
    "    output['seed_models'] = seed_models\n",
    "\n",
    "    return output, name\n",
    "\n",
    "\n",
    "def trykey(params, key, default):\n",
    "    try:\n",
    "        return params[key]\n",
    "    except KeyError:\n",
    "        params[key] = default\n",
    "        return params[key]\n",
    "\n",
    "\n",
    "def baselines(BaselinesList, X, y, ParamsList, seed=63, TreatCols=None, id='', timeit=False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X, colnamesX: potential causes and their names\n",
    "        Z, colnamesZ: confounders and their names\n",
    "        y: 01 outcome\n",
    "        causes: name of the potential causes\n",
    "    \"\"\"\n",
    "\n",
    "    if TreatCols is None:\n",
    "        TreatCols = list(range(X.shape[1]))\n",
    "\n",
    "    # check if binary treatments\n",
    "    X01 = X.copy()\n",
    "    for col in TreatCols:\n",
    "        a = X01.iloc[:, col]\n",
    "        if not ((a == 0) | (a == 1)).all():\n",
    "            mean_v = np.mean(X01.iloc[:, col])\n",
    "            X01.iloc[:, col] = [1 if i > mean_v else 0 for i in X01.iloc[:, col]]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    X_train, X_test, y_train, y_test, X_train01, X_test01 = train_test_split(X, y, X01,\n",
    "                                                                             test_size=0.33, random_state=seed)\n",
    "    coef_table = pd.DataFrame(columns=['causes'])\n",
    "    coef_table['causes'] = ['T' + str(i) for i in range(len(TreatCols))]\n",
    "    times, f1_test = {}, {}\n",
    "\n",
    "    if 'DA' in BaselinesList:\n",
    "        start_time = time.time()\n",
    "        from deconfounder import deconfounder_algorithm as DA\n",
    "        ParamsList['DA']['k'] = trykey(ParamsList['DA'], 'k', 15)  # if exploring multiple latent sizes\n",
    "        for k in ParamsList['DA']['k']:\n",
    "            if len(ParamsList['DA']['k']) > 1:\n",
    "                coln = 'DA_' + str(id) + str(k)\n",
    "            else:\n",
    "                coln = 'DA'\n",
    "            model_da = DA(X_train, X_test, y_train, y_test, k, print_=False)\n",
    "            ParamsList['DA']['class_weight'] = trykey(ParamsList['DA'], 'class_weight', {0: 1, 1: 1})\n",
    "            coef, coef_continuos, roc, f1_test['DA'] = model_da.fit(class_weight=ParamsList['DA']['class_weight'])\n",
    "            coef_table[coln] = coef_continuos[TreatCols]\n",
    "        times['DA'] = time.time() - start_time\n",
    "        print('\\nDone!')\n",
    "\n",
    "    if 'BART' in BaselinesList:\n",
    "        start_time = time.time()\n",
    "        from bart import BART as BART\n",
    "        model_bart = BART(X_train01, X_test01, y_train, y_test)\n",
    "        ParamsList['BART']['n_trees'] = trykey(ParamsList['BART'], 'n_trees', 50)\n",
    "        ParamsList['BART']['n_burn'] = trykey(ParamsList['BART'], 'n_burn', 100)\n",
    "        model_bart.fit(n_trees=ParamsList['BART']['n_trees'], n_burn=ParamsList['BART']['n_burn'], print_=False)\n",
    "        print('...... predictions')\n",
    "        coef_table['BART'], f1_test['BART'] = model_bart.cate(TreatCols, print_=False)\n",
    "        times['BART'] = time.time() - start_time\n",
    "        print('\\nDone!')\n",
    "\n",
    "    if 'CEVAE' in BaselinesList:\n",
    "        print('\\n\\n Learner: CEVAE')\n",
    "        start_time = time.time()\n",
    "        from cevae import CEVAE as CEVAE\n",
    "        print('Note: Treatments should be the first columns of X')\n",
    "        ParamsList['CEVAE']['epochs'] = trykey(ParamsList['CEVAE'], 'epochs', 100)\n",
    "        ParamsList['CEVAE']['batch'] = trykey(ParamsList['CEVAE'], 'batch', 200)\n",
    "        ParamsList['CEVAE']['z_dim'] = trykey(ParamsList['CEVAE'], 'z_dim', 5)\n",
    "\n",
    "        confeatures, binfeatures = [], []\n",
    "        for col in range(X_train01.shape[1]):\n",
    "            a = X_train01.iloc[:, col]\n",
    "            if not ((a == 0) | (a == 1)).all():\n",
    "                confeatures.append(col)\n",
    "            else:\n",
    "                binfeatures.append(col)\n",
    "\n",
    "        print('... length con and bin features', len(confeatures), len(binfeatures))\n",
    "        model_cevae = CEVAE(X_train01, X_test01, y_train, y_test, TreatCols,\n",
    "                            binfeats=binfeatures, contfeats=confeatures,\n",
    "                            epochs=ParamsList['CEVAE']['epochs'],\n",
    "                            batch=ParamsList['CEVAE']['batch'],\n",
    "                            z_dim=ParamsList['CEVAE']['z_dim'])\n",
    "        coef_table['CEVAE'], f1_test['CEVAE'] = model_cevae.fit_all(print_=False)\n",
    "        times['CEVAE'] = time.time() - start_time\n",
    "        print('\\nDone!')\n",
    "\n",
    "    if not timeit:\n",
    "        return coef_table\n",
    "    else:\n",
    "        return coef_table, times, f1_test\n",
    "\n",
    "\n",
    "def organize_output(experiments, true_effect, exp_time=None, f1_scores=None):\n",
    "    \"\"\"\n",
    "    Important: experiments, experiments times and f1 scores should be in the same order\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiments\n",
    "    true_effect\n",
    "    exp_time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    Treatments = experiments['causes']\n",
    "    experiments.set_index('causes', inplace=True)\n",
    "    experiments['TrueTreat'] = true_effect\n",
    "    Treatments_cate = np.transpose(experiments)\n",
    "    BaselinesNames = experiments.columns\n",
    "    mae = []\n",
    "    for col in BaselinesNames:\n",
    "        dif = np.abs(experiments[col] - experiments['TrueTreat'])\n",
    "        mae.append(np.nanmean(dif))\n",
    "    output = pd.DataFrame({'Method': BaselinesNames, 'MAE': mae})\n",
    "    exp_time['TrueTreat'] = 0\n",
    "    f1_scores['TrueTreat'] = 0\n",
    "    if f1_scores is not None:\n",
    "        output['F1_Test'] = [f1_scores[m] for m in output['Method'].values]\n",
    "    if exp_time is not None:\n",
    "        output['Time(s)'] = [exp_time[m] for m in output['Method'].values]\n",
    "\n",
    "    out = pd.DataFrame(Treatments_cate, columns=Treatments)\n",
    "    out.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return pd.concat((output, out), 1)\n",
    "\n",
    "\n",
    "colab = False\n",
    "notebook = True\n",
    "arg = {'config_path': 'config1.yaml',\n",
    "       'seed_models': 10,\n",
    "       'seed_data': 5,\n",
    "       }\n",
    "if colab: \n",
    "    arg['path'] = '/content/'\n",
    "    arg['config_path'] = arg['path']+arg['config_path']\n",
    "else: \n",
    "    arg['path'] = ''\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    if notebook:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Cuda Availble:\", torch.cuda.is_available(), \" device: \", device)\n",
    "        for j in range(arg['seed_data']):\n",
    "            print('Data',j)\n",
    "            for i in range(arg['seed_models']):\n",
    "                print('Models',i)\n",
    "                if i == 0 and j == 0:\n",
    "                    output, name = main(config_path=arg['config_path'], seed_models=i, seed_data=j)\n",
    "                else:\n",
    "                    output_, name = main(config_path=arg['config_path'], seed_models=i, seed_data=j)\n",
    "                    output = pd.concat([output, output_], 0, ignore_index=True)\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Cuda Availble:\", torch.cuda.is_available(), \" device: \", device)\n",
    "        for j in range(sys.argv[3]):\n",
    "            print('Data', j)\n",
    "            for i in range(sys.argv[2]):\n",
    "                print('Models', i)\n",
    "                if i == 0:\n",
    "                    output, name = main(config_path=sys.argv[1], seed_models=i, seed_data=j)\n",
    "                else:\n",
    "                    output_, name = main(config_path=sys.argv[1], seed_models=i, seed_data=j)\n",
    "                    output = pd.concat([output, output_], 0, ignore_index=True)\n",
    "\n",
    "    output.to_csv(name)\n",
    "    end_time = time.time() - start_time\n",
    "    end_time_m = end_time / 60\n",
    "    end_time_h = end_time_m / 60\n",
    "    print(\"Time ------ {} min / {} hours ------\".format(end_time_m, end_time_h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-carry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-andrew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-control",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-communications",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-arthur",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-marriage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "reduced-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moral-actor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_probability as tp\n",
    "tp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-importance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "institutional-export",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "analyzed-network",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-sleeping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-dollar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-harvest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-actor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-shadow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-policy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-honor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-europe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-domestic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-cologne",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
