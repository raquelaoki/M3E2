{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M3E2_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrIcSLnIJDHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4102458c-52f0-4591-da49-f182fc89efa5"
      },
      "source": [
        "!git clone https://github.com/raquelaoki/ParKCa.git\n",
        "!git clone https://github.com/raquelaoki/CompBioAndSimulated_Datasets.git\n",
        "!git clone https://github.com/JakeColtman/bartpy.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ParKCa'...\n",
            "remote: Enumerating objects: 488, done.\u001b[K\n",
            "remote: Counting objects: 100% (488/488), done.\u001b[K\n",
            "remote: Compressing objects: 100% (341/341), done.\u001b[K\n",
            "remote: Total 1084 (delta 336), reused 272 (delta 145), pack-reused 596\u001b[K\n",
            "Receiving objects: 100% (1084/1084), 3.21 MiB | 10.30 MiB/s, done.\n",
            "Resolving deltas: 100% (716/716), done.\n",
            "Cloning into 'CompBioAndSimulated_Datasets'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 108 (delta 64), reused 66 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (108/108), 127.34 KiB | 6.70 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Cloning into 'bartpy'...\n",
            "remote: Enumerating objects: 1971, done.\u001b[K\n",
            "remote: Total 1971 (delta 0), reused 0 (delta 0), pack-reused 1971\u001b[K\n",
            "Receiving objects: 100% (1971/1971), 11.97 MiB | 2.15 MiB/s, done.\n",
            "Resolving deltas: 100% (1329/1329), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-GR1ooJ1Fjg",
        "outputId": "1d725304-fc1d-4152-af2b-79d2d08cc77d"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "sys.path.insert(0, 'src/')\n",
        "sys.path.insert(0,'bartpy/')\n",
        "sys.path.insert(0,'ParKCa/src/')\n",
        "from ParKCa.src.train import *\n",
        "from CompBioAndSimulated_Datasets.simulated_data_multicause import *\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te2_h7n5FIpn"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMWOUXwMomAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1458d498-9bb5-461e-d2b4-aa19d9b8f3f7"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "sys.path.insert(0, 'src/')\n",
        "sys.path.insert(0,'bartpy/')\n",
        "sys.path.insert(0,'ParKCa/src/')\n",
        "from ParKCa.src.train import *\n",
        "from CompBioAndSimulated_Datasets.simulated_data_multicause import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def trykey(params,key,default):\n",
        "    try:\n",
        "        return params[key]\n",
        "    except KeyError:\n",
        "        params[key] = default\n",
        "        return params[key]\n",
        "\n",
        "\n",
        "\n",
        "m3e2 = True\n",
        "if m3e2: \n",
        "\n",
        "  import time\n",
        "  import yaml\n",
        "  import model_m3e2 as m3e2\n",
        "  #import torch.nn as nn\n",
        "  #import torch.nn.functional as F\n",
        "  #from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "  #from torch import optim, Tensor\n",
        "\n",
        "\n",
        "  def main(config_path):\n",
        "      \"\"\"Start: Parameters Loading\"\"\"\n",
        "      with open(config_path) as f:\n",
        "          config = yaml.safe_load(f)\n",
        "      params = config['parameters']\n",
        "\n",
        "      try:\n",
        "          SEED = params[\"SEED\"]\n",
        "      except KeyError:\n",
        "          SEED = 10\n",
        "\n",
        "      # Fix numpy seed for reproducibility\n",
        "      np.random.seed(SEED)\n",
        "      # Fix random seed for reproducibility\n",
        "      random.seed(SEED)\n",
        "      # Fix Torch graph-level seed for reproducibility\n",
        "      torch.manual_seed(SEED)\n",
        "      print('seed',SEED)\n",
        "\n",
        "      if 'gwas' in params['data']:\n",
        "          sdata_gwas = gwas_simulated_data(prop_tc=0.05, pca_path='/content/CompBioAndSimulated_Datasets/data/tgp_pca2.txt')\n",
        "          X, y, y01, treatement_columns, treatment_effects, group = sdata_gwas.generate_samples()\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y01, test_size=0.33, random_state=SEED)\n",
        "          print('... Target - proportion of 1s', np.sum(y01)/len(y01) )\n",
        "          # Split X1, X2 on GWAS\n",
        "          X1_cols = []\n",
        "          X2_cols = range(X.shape[1]-len(treatement_columns))\n",
        "          # TODO: add other baselines here to run everything on the same train/testing sets\n",
        "\n",
        "          data_nnl = m3e2.data_nn(X_train.values, X_test.values, y_train, y_test, treatement_columns,\n",
        "                                  treatment_effects[treatement_columns], X1_cols, X2_cols)\n",
        "          loader_train, loader_val, loader_test, num_features = data_nnl.loader(params['suffle'],params['batch_size'],SEED)\n",
        "          params['pos_weights'] = data_nnl.treat_weights\n",
        "          params['pos_weight_y'] = trykey(params,'pos_weight_y',1)\n",
        "          params['hidden1'] = trykey(params,'hidden1',64)\n",
        "          params['hidden2'] = trykey(params,'hidden2',8)\n",
        "          cate_m3e2 = m3e2.fit_nn(loader_train, loader_val, loader_test, params, treatement_columns, num_features,\n",
        "                                X1_cols, X2_cols)\n",
        "          print('... CATE')\n",
        "          cate = pd.DataFrame({'CATE_M3E2':cate_m3e2,'True_Effect':treatment_effects[treatement_columns]})\n",
        "          print(cate)\n",
        "          dif = cate_m3e2-treatment_effects[treatement_columns]\n",
        "          print('MAE',np.abs(dif).mean())\n",
        "      if 'copula' in params['data']:\n",
        "          sdata_copula = copula_simulated_data()\n",
        "          X, y, y01, treatement_columns, treatment_effects = sdata_copula.generate_samples()\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y01, test_size=0.33, random_state=SEED)\n",
        "          print('... Target - proportion of 1s', np.sum(y01) / len(y01))\n",
        "          # Split X1, X2 on GWAS\n",
        "          X1_cols = []\n",
        "          X2_cols = range(X.shape[1] - len(treatement_columns))\n",
        "          # TODO: add other baselines here to run everything on the same train/testing sets\n",
        "          data_nnl = m3e2.data_nn(X_train, X_test, y_train, y_test, treatement_columns,\n",
        "                                  treatment_effects, X1_cols, X2_cols)\n",
        "          loader_train, loader_val, loader_test, num_features = data_nnl.loader(params['suffle'], params['batch_size'],\n",
        "                                                                                SEED)\n",
        "          params['pos_weights'] = data_nnl.treat_weights\n",
        "          params['pos_weight_y'] = trykey(params,'pos_weight_y',1)\n",
        "          params['hidden1'] = trykey(params,'hidden1',6)\n",
        "          params['hidden2'] = trykey(params,'hidden2',3)\n",
        "          cate_m3e2 = m3e2.fit_nn(loader_train, loader_val, loader_test, params, treatement_columns, num_features,\n",
        "                                  X1_cols, X2_cols)\n",
        "          print('... CATE')\n",
        "          cate = pd.DataFrame({'CATE_M3E2': cate_m3e2, 'True_Effect': treatment_effects})\n",
        "          print(cate)\n",
        "          dif = cate_m3e2 - treatment_effects\n",
        "          print('MAE', np.abs(dif).mean())\n",
        "      if 'gwas' not in params['data'] and 'copula' not in params['data']:\n",
        "          print('ERRROR! \\nDataset not recognized. \\nChange the parameter data in your config.yaml file to gwas or copula.')  \n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      start_time = time.time()\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      print(\"Cuda Availble:\", torch.cuda.is_available(), \" device: \", device)\n",
        "      main(config_path='/content/config2.yaml')\n",
        "      end_time = time.time() - start_time\n",
        "      end_time_m = end_time / 60\n",
        "      end_time_h = end_time_m / 60\n",
        "      print(\"Time ------ {} min / {} hours ------\".format(end_time_m, end_time_h))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Cuda Availble: True  device:  cuda\n",
            "seed 10\n",
            "Copula simulated data initialized!\n",
            "... Treatments: (10000, 4)\n",
            "... Confounders: (10000, 10)\n",
            "... Target (y): 0.6766\n",
            "Data Simulation Done!\n",
            "[[-1.   -1.    1.   -0.06]]\n",
            "[[ 1.02752294  0.25688073 -0.20550459  0.10275229]]\n",
            "... Target - proportion of 1s 0.6766\n",
            "Weights [ -33.79254788 -635.89574239    8.07677019  -50.02857128]\n",
            "M3E2: Train Shape  (6700, 10) (6700, 4)\n",
            "... Model initialization done!\n",
            "... Training\n",
            "......  0  \n",
            "Train: loss  27.78 1.12 metric  [4.92599201 1.20046854 0.81006777 1.1302768  0.61151283] \n",
            "Val: loss  17.24 metric  [5.16339493 1.26362026 0.94801182 1.09876001 0.5961962 ]\n",
            "......  30  \n",
            "Train: loss  18.17 1.01 metric  [2.6418469  1.08863413 0.74769765 1.0547576  0.67761764] \n",
            "Val: loss  11.76 metric  [2.62610793 1.15354431 0.75240564 0.99653137 0.66956957]\n",
            "......  60  \n",
            "Train: loss  16.64 0.96 metric  [2.24643588 1.1452136  0.72321409 1.07581091 0.64684186] \n",
            "Val: loss  10.03 metric  [2.19348478 1.14548004 0.75412214 0.9904353  0.64366867]\n",
            "......  90  \n",
            "Train: loss  16.09 0.95 metric  [1.71297383 1.05150807 0.72621477 1.01176953 0.66760649] \n",
            "Val: loss  9.46 metric  [2.10810924 1.1399194  0.7443592  0.98666728 0.66281281]\n",
            "......  120  \n",
            "Train: loss  15.81 0.95 metric  [2.0773778  1.04705036 0.77371538 1.09181988 0.66699679] \n",
            "Val: loss  9.28 metric  [2.04300547 1.13536549 0.73041177 0.98715615 0.67094595]\n",
            "......  150  \n",
            "Train: loss  15.68 0.94 metric  [2.06257915 1.10005081 0.76292074 0.96499634 0.65900552] \n",
            "Val: loss  9.22 metric  [2.01328635 1.12277901 0.73031634 0.97695011 0.66093594]\n",
            "......  180  \n",
            "Train: loss  15.83 0.94 metric  [1.99126089 1.07779741 0.83924288 0.99805623 0.65634815] \n",
            "Val: loss  9.31 metric  [2.08604527 1.11858654 0.73324811 0.98071188 0.66028529]\n",
            "......  210  \n",
            "Train: loss  15.56 0.94 metric  [1.47559726 1.04105997 0.6377421  1.10080051 0.66146271] \n",
            "Val: loss  9.26 metric  [2.07819796 1.11947417 0.73026383 0.97976977 0.65800801]\n",
            "......  240  \n",
            "Train: loss  15.66 0.93 metric  [2.29808545 1.09514761 0.88852096 1.03247023 0.6604509 ] \n",
            "Val: loss  9.18 metric  [2.06181312 1.08816183 0.72829604 0.97456217 0.65065065]\n",
            "......  270  \n",
            "Train: loss  15.71 0.93 metric  [2.21080542 1.1179527  0.71522677 0.84569985 0.66420826] \n",
            "Val: loss  9.19 metric  [2.05234075 1.09827769 0.72797114 0.97855586 0.64784785]\n",
            "......  300  \n",
            "Train: loss  15.79 0.93 metric  [1.90998566 1.07540703 0.94907683 1.01356244 0.65550316] \n",
            "Val: loss  8.96 metric  [1.86701918 1.09967124 0.72283012 0.97771209 0.65568068]\n",
            "......  330  \n",
            "Train: loss  15.86 0.93 metric  [2.39735508 1.06635892 0.68605775 1.01550579 0.65973835] \n",
            "Val: loss  9.42 metric  [2.13538694 1.09215951 0.73754692 0.97022474 0.64146647]\n",
            "......  360  \n",
            "Train: loss  15.59 0.93 metric  [1.6485939  0.99237043 0.77187103 0.89377624 0.6535495 ] \n",
            "Val: loss  9.4 metric  [2.11850119 1.07475352 0.73923916 0.97183251 0.64006507]\n",
            "......  390  \n",
            "Train: loss  15.7 0.93 metric  [1.7914654  0.8825025  0.72325212 0.97324127 0.65317107] \n",
            "Val: loss  9.24 metric  [2.07798195 1.07909262 0.73625654 0.9736973  0.6525025 ]\n",
            "... Loading Best validation (epoch  80 )\n",
            "... Final Metrics - Target\n",
            "[[ 60 109]\n",
            " [  8 323]]\n",
            "...... Train :  0.815\n",
            "[[ 205  335]\n",
            " [  27 1083]]\n",
            "...... Val :  0.824\n",
            "[[ 183  348]\n",
            " [  26 1093]]\n",
            "...... Test :  0.817\n",
            "Outcome Y [ 0.2690331   0.17402193  0.15935758  0.04636333  0.96620727  0.7762149\n",
            " -1.1186823   0.066231  ]\n",
            "Bias  [0.]\n",
            "... CATE\n",
            "   CATE_M3E2  True_Effect\n",
            "0   0.269033     0.027523\n",
            "1   0.174022    -0.743119\n",
            "2   0.159358     0.794495\n",
            "3   0.046363     0.042752\n",
            "MAE 0.4493500592600588\n",
            "Time ------ 1.6392383098602294 min / 0.02732063849767049 hours ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTLMrPh_POo8"
      },
      "source": [
        "TODO: \n",
        "1. in copula, treat is continuous - fine for DA, need change our loss/propensity; \n",
        "2. A treatment is dominating the training set\n",
        "3. Add RMSE with DA, BART, CEVAE results for GWAS, copula. \n",
        "  a.Create a table, single x multi, with empty spaces for future models \n",
        "  b. What I'm evaluating? Adding more confoudners robusteness, adding more treatments (T) (add running time tracker), adding more samples (n). \n",
        "4. Plots "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE2a1DW7D4pP"
      },
      "source": [
        "def baselines(BaselinesList, X, y, ParamsList, seed=63, TreatCols=None, id='', timeit=False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        X, colnamesX: potential causes and their names\n",
        "        Z, colnamesZ: confounders and their names\n",
        "        y: 01 outcome\n",
        "        causes: name of the potential causes\n",
        "    \"\"\"\n",
        "    if TreatCols is None:\n",
        "        TreatCols = list(range(X.shape[1]))\n",
        "\n",
        "    # check if binary treatments\n",
        "    X01 = X.copy()\n",
        "    for col in TreatCols:\n",
        "        a = X01.iloc[:,col]\n",
        "        if not ((a == 0) | (a == 1)).all():\n",
        "            mean_v = np.mean(X01.iloc[:,col])\n",
        "            X01.iloc[:,col] = [1 if i > mean_v else 0 for i in X01.iloc[:,col]]\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    X_train, X_test, y_train, y_test, X_train01, X_test01 = train_test_split(X, y, X01,\n",
        "                                                                             test_size=0.33, random_state=seed)\n",
        "    coef_table = pd.DataFrame(columns=['causes'])\n",
        "    coef_table['causes'] = ['T' + str(i) for i in range(len(TreatCols))]\n",
        "    times = {}\n",
        "\n",
        "    if 'DA' in BaselinesList:\n",
        "        print('\\n\\nBaseline: DA')\n",
        "        start_time = time.time()\n",
        "        from deconfounder import deconfounder_algorithm as DA\n",
        "        ParamsList['DA']['k'] = trykey(ParamsList['DA'],'k',15) # if exploring multiple latent sizes\n",
        "        for k in ParamsList['DA']['k']:\n",
        "            if len(ParamsList['DA']['k']) > 1:\n",
        "                coln = 'DA_' + str(id) + str(k)\n",
        "            else:\n",
        "                coln = 'DA'\n",
        "            model_da = DA(X_train, X_test, y_train, y_test, k)\n",
        "            ParamsList['DA']['class_weight'] = trykey(ParamsList['DA'], 'class_weight', {0:1,1:1})\n",
        "            coef, coef_continuos, roc = model_da.fit(class_weight=ParamsList['DA']['class_weight'])\n",
        "            coef_table[coln] = coef_continuos[0:len(TreatCols)]\n",
        "        times['DA'] = time.time() - start_time\n",
        "        print('Done!')\n",
        "\n",
        "    if 'BART' in BaselinesList:\n",
        "        print('\\n\\nLearner: BART')\n",
        "        start_time = time.time()\n",
        "        from bart import BART as BART\n",
        "        model_bart = BART(X_train01, X_test01, y_train, y_test)\n",
        "        ParamsList['BART']['n_trees'] = trykey(ParamsList['BART'], 'n_trees', 50)\n",
        "        ParamsList['BART']['n_burn'] = trykey(ParamsList['BART'], 'n_burn', 100)\n",
        "        model_bart.fit(n_trees=ParamsList['BART']['n_trees'], n_burn=ParamsList['BART']['n_burn'])\n",
        "        print('...... predictions')\n",
        "        coef_table['BART'] = model_bart.cate(TreatCols)\n",
        "        times['BART'] = time.time() - start_time\n",
        "        print('Done!')\n",
        "\n",
        "    if 'CEVAE' in BaselinesList:\n",
        "        print('\\n\\n Learner: CEVAE')\n",
        "        start_time = time.time()\n",
        "        from cevae import CEVAE as CEVAE\n",
        "        print('Note: Treatments should be the first columns of X')\n",
        "        ParamsList['CEVAE']['epochs'] = trykey(ParamsList['CEVAE'], 'epochs', 100)\n",
        "        ParamsList['CEVAE']['batch'] = trykey(ParamsList['CEVAE'], 'batch', 200)\n",
        "        ParamsList['CEVAE']['z_dim'] = trykey(ParamsList['CEVAE'], 'z_dim', 5)\n",
        "\n",
        "        confeatures, binfeatures = [],[]\n",
        "        for col in range(X_train01.shape[1]):\n",
        "            a = X_train01.iloc[:,col]\n",
        "            if not ((a == 0) | (a == 1)).all():\n",
        "              confeatures.append(col)\n",
        "            else:\n",
        "              binfeatures.append(col)\n",
        "\n",
        "        print('... length con and bin features', len(confeatures), len(binfeatures))\n",
        "        model_cevae = CEVAE(X_train01, X_test01, y_train, y_test, TreatCols, \n",
        "                            binfeats=binfeatures, contfeats=confeatures,\n",
        "                            epochs=ParamsList['CEVAE']['epochs'], \n",
        "                            batch=ParamsList['CEVAE']['batch'],\n",
        "                            z_dim=ParamsList['CEVAE']['z_dim'])\n",
        "        cate = model_cevae.fit_all()\n",
        "        coef_table['CEVAE'] = cate\n",
        "        times['CEVAE'] = time.time() - start_time\n",
        "        print('Done!')\n",
        "\n",
        "    if not timeit:\n",
        "        return coef_table\n",
        "    else:\n",
        "        return coef_table, times\n",
        "\n",
        "def organize_output(experiments, true_effect, exp_time):\n",
        "    experiments['TrueTreat'] = true_effect\n",
        "    experiments.set_index('causes', inplace=True)\n",
        "    BaselinesNames = experiments.columns\n",
        "\n",
        "    mae = []\n",
        "    for col in BaselinesNames:\n",
        "        mae.append(np.sum(np.abs(experiments[col] - experiments['TrueTreat'])) / experiments.shape[0])\n",
        "\n",
        "    output = pd.DataFrame({'Method': BaselinesNames, 'MAE': mae})\n",
        "    exp_time['TrueTreat'] = 0\n",
        "    output['Time(s)'] = [exp_time[m] for m in output['Method'].values]\n",
        "    print(experiments, '\\n', output)\n",
        "    return experiments, output\n",
        "\n",
        "def trykey(params,key,default):\n",
        "    try:\n",
        "        return params[key]\n",
        "    except KeyError:\n",
        "        params[key] = default\n",
        "        return params[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOF-MAg7dFOC"
      },
      "source": [
        "params = {'DA':{'k':[15]},\n",
        "          'BART':{'n_trees':15,'n_burn':100},\n",
        "          'CEVAE':{'num_epochs':100,'batch': 200,'z_dim':10},\n",
        "          'seed':10}\n",
        "\n",
        "sdata_gwas = gwas_simulated_data(prop_tc=0.05, seed=params['seed'], pca_path='/content/CompBioAndSimulated_Datasets/data/tgp_pca2.txt')\n",
        "X, y, y01, treatement_columns, treatment_effects,group  = sdata_gwas.generate_samples()\n",
        "\n",
        "#'DA','BART'\n",
        "experiments, exp_time = baselines(['CEVAE','DA','BART'],pd.DataFrame(X),y01, params,\n",
        "                       TreatCols = treatement_columns, timeit=True, seed=params['seed']) #'CEVAE',\n",
        "\n",
        "experiments, output = organize_output(experiments, treatment_effects[treatement_columns], exp_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO01mHhI-m1v"
      },
      "source": [
        "params = {'DA':{'k':[15], 'class_weight':{0:1, 1:1}},\n",
        "          'BART':{'n_trees':3,'n_burn':50},\n",
        "          'CEVAE':{'num_epochs':100,'batch': 200,'z_dim':5},\n",
        "          'seed':10}\n",
        "          \n",
        "sdata_copula = copula_simulated_data(seed = params['seed'])\n",
        "X, y, y01, treatement_columns, treatment_effects = sdata_copula.generate_samples()\n",
        "\n",
        "experiments, exp_time = baselines(['BART','DA','CEVAE'],pd.DataFrame(X),y01, params,\n",
        "                       TreatCols = treatement_columns, timeit=True) #'CEVAE',\n",
        "\n",
        "experiments, output = organize_output(experiments, treatment_effects[treatement_columns], exp_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbylg2JYXUB8"
      },
      "source": [
        "# ADD IHDP DATASET "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-amed4oY0T7"
      },
      "source": [
        "a = np.array([0.176609,0.057250 ,0.043792 ])\n",
        "b = np.array([0.027523,-0.743119,0.042752 ])\n",
        "np.sum(np.abs(a-b))/2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}